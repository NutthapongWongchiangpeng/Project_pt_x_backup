{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "from lxml import etree\n",
    "#import numpy as np\n",
    "#import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "#import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "    raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils.np_box_ops import iou\n",
    "\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_dir', '', 'Location of root directory')\n",
    "\n",
    "flags.DEFINE_string('output_dir', 'C:/Users/LOLiCON/Desktop/obj_detect', 'Path output')\n",
    "\n",
    "\n",
    "flags.DEFINE_string('label_map_path', 'C:/Users/LOLiCON/Desktop/obj_detect/detrac_label_map.pbtxt',\n",
    "                           'Path to label map proto.')\n",
    "\n",
    "#\n",
    "FLAGS = flags.FLAGS\n",
    "training_path='D:/DataSet/Insight-MVT_Annotation_Train'\n",
    "\n",
    "\n",
    "data_dir_path='D:/DataSet'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tf_example(data,\n",
    "                       label_map_dict,\n",
    "                       image_subdirectory,\n",
    "                       bbox,\n",
    "                       ignore_difficult_instances=False):\n",
    "    img_path = os.path.join(image_subdirectory,data)\n",
    "    \n",
    "    with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format not JPEG')\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    width,height = image.size\n",
    "    xmins = []\n",
    "    ymins = []\n",
    "    xmaxs = []\n",
    "    ymaxs = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    orientation=[]\n",
    "    \n",
    "    #print(img_path)\n",
    "    for target in bbox.findall('target'):\n",
    "        a = target.find('box')\n",
    "        b = target.find('attribute')\n",
    "        \n",
    "        xmin= (float(a.attrib.get('left')))\n",
    "        ymin= (float(a.attrib.get('top')))\n",
    "        xmax= (float(a.attrib.get('width')) + xmin)\n",
    "        ymax= (float(a.attrib.get('height')) +ymin)\n",
    "            \n",
    "        \n",
    "        xmins.append(xmin / width)\n",
    "        ymins.append(ymin / height)\n",
    "        xmaxs.append(xmax / width)\n",
    "        ymaxs.append(ymax / height)\n",
    "        class_name = b.attrib.get('vehicle_type')\n",
    "        classes_text.append(class_name.encode('utf8'))\n",
    "        classes.append(label_map_dict[class_name])\n",
    "        truncated.append(float(b.attrib.get('truncation_ratio')))\n",
    "        #print(xmin,ymin,xmax,ymax)\n",
    "\n",
    "    \n",
    "    #print(xmins)\n",
    "    #print(img_path)\n",
    "    #print(class_name)\n",
    "    feature_dict = {\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(\n",
    "          data.encode('utf8')),\n",
    "      'image/source_id': dataset_util.bytes_feature(\n",
    "          data.encode('utf8')),\n",
    "      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/truncated': dataset_util.float_list_feature(truncated),\n",
    "  }\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml_tree(xml_path):\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bb=''\n",
    "    for weather in root.findall('sequence_attribute'):\n",
    "        bb=weather.attrib.get('sence_weather')\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tf_record(label_map_dict,\n",
    "                     annotations_dir,\n",
    "                     image_dir,\n",
    "                     excalibur,\n",
    "                     data_folder):\n",
    "\n",
    "    \n",
    "\n",
    "    xml_path = os.path.join(annotations_dir, data_folder+'.xml')\n",
    "\n",
    "    print(xml_path)\n",
    "    train_output_path = os.path.join(FLAGS.output_dir, 'detrac_train_only_1.record')\n",
    "    val_output_path = os.path.join(FLAGS.output_dir, 'detrac_val_only_1.record')\n",
    "        \n",
    "        #with tf.gfile.GFile(xml_path, 'r') as fid:\n",
    "    \n",
    "            #xml_str = fid.read()\n",
    "        #xml = etree.fromstring(xml_str)\n",
    "        #data = dataset_util.recursive_parse_xml_to_dict(xml)\n",
    "        #print(data['sequence'])\n",
    "    txtfile=dataset_util.read_examples_list(excalibur+'/'+data_folder+'.txt')\n",
    "    \n",
    "    num_examples = len(txtfile)\n",
    "    num_train = int(0.8 * num_examples)\n",
    "    list_train_val=[]\n",
    "    count_train_val=0\n",
    "    for a in txtfile:\n",
    "        if(count_train_val<num_train):\n",
    "            list_train_val.append(1) #train \n",
    "        else:\n",
    "            list_train_val.append(0) # val\n",
    "        count_train_val+=1\n",
    "    random.seed(42)\n",
    "    random.shuffle(list_train_val)\n",
    "    \n",
    "    \n",
    "    print(data_folder)\n",
    "    aa=[]  \n",
    "    img_list=[]\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    i=0 \n",
    "    j=0\n",
    "    for frame in root.findall('frame'):\n",
    "        num_img= int(float(frame.attrib.get('num')))\n",
    "        target_list = frame.find('target_list')\n",
    "        aa.append(target_list)\n",
    "        img_list.append(num_img)\n",
    "        #print(num_img)\n",
    "    \n",
    "    try:\n",
    "           \n",
    "        for data in txtfile: \n",
    "            if((i+1)==img_list[j]):\n",
    "                tf_example = dict_to_tf_example(data, label_map_dict, image_dir,aa[j])\n",
    "                \n",
    "                if(j<len(img_list)-1):\n",
    "                    j=j+1\n",
    "            \n",
    "                if (list_train_val[i]==0):\n",
    "                    writer = tf.python_io.TFRecordWriter(val_output_path)\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "                    writer.close()\n",
    "                else:\n",
    "                    writer = tf.python_io.TFRecordWriter(train_output_path)\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "                    writer.close()\n",
    "            i=i+1\n",
    "    except ValueError:\n",
    "        logging.warning('Invalid example: %s, ignoring.', xml_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunny\n",
      "1\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20011.xml\n",
      "MVI_20011\n",
      "sunny\n",
      "2\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20012.xml\n",
      "MVI_20012\n",
      "sunny\n",
      "3\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20032.xml\n",
      "MVI_20032\n",
      "sunny\n",
      "4\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20033.xml\n",
      "MVI_20033\n",
      "sunny\n",
      "5\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20034.xml\n",
      "MVI_20034\n",
      "sunny\n",
      "6\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20035.xml\n",
      "MVI_20035\n",
      "sunny\n",
      "7\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20051.xml\n",
      "MVI_20051\n",
      "sunny\n",
      "8\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20052.xml\n",
      "MVI_20052\n",
      "sunny\n",
      "9\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20061.xml\n",
      "MVI_20061\n",
      "sunny\n",
      "10\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20062.xml\n",
      "MVI_20062\n",
      "sunny\n",
      "11\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20063.xml\n",
      "MVI_20063\n",
      "sunny\n",
      "12\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20064.xml\n",
      "MVI_20064\n",
      "sunny\n",
      "13\n",
      "train\n",
      "D:/DataSet\\DETRAC-Train-Annotations-XML\\MVI_20065.xml\n",
      "MVI_20065\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(_):\n",
    "    data_dir = FLAGS.data_dir\n",
    "    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n",
    "\n",
    "   \n",
    "    logging.info('Reading from DETRAC dataset.')\n",
    "    \n",
    "    \n",
    "    annotations_dir = os.path.join(data_dir_path, 'DETRAC-Train-Annotations-XML')\n",
    "\n",
    "  # Test images are not included in the downloaded data set, so we shall perform\n",
    "  # our own split.\n",
    "\n",
    "    \n",
    "    #use 1 view only\n",
    "    excalibur=data_dir_path+'/ua-text_file_front'\n",
    "    \n",
    "#    csv=pd.read_csv(excalibur+'/Insight-MVT_Annotation_Train.txt')\n",
    "    txtfile=dataset_util.read_examples_list(excalibur+'/Insight-MVT_Annotation_Train.txt')\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for data_folder in txtfile:\n",
    "        weather=xml_tree(os.path.join(annotations_dir, data_folder+'.xml'))\n",
    "        if (weather=='sunny' or weather=='cloudy'):\n",
    "            print(weather)\n",
    "            count += 1\n",
    "            print(count)\n",
    "            print('train')\n",
    "            image_dir = os.path.join(training_path, data_folder)\n",
    "            create_tf_record(label_map_dict, annotations_dir,image_dir,excalibur,data_folder)\n",
    "        \n",
    "  # TODO(user): Write code to read in your dataset to examples variable\n",
    "    \n",
    "#  for example in examples:\n",
    "#    tf_example = create_tf_example(example)\n",
    "#    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "#  writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
