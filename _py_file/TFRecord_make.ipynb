{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "from lxml import etree\n",
    "#import numpy as np\n",
    "#import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "#import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "if tf.__version__ < '1.4.0':\n",
    "    raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils.np_box_ops import iou\n",
    "\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_dir', '', 'Location of root directory')\n",
    "\n",
    "flags.DEFINE_string('output_dir', 'L:/Dataset/Obj_detec', 'Path output')\n",
    "\n",
    "\n",
    "flags.DEFINE_string('label_map_path', 'L:/Dataset/Obj_detec/detrac_label_map.pbtxt',\n",
    "                           'Path to label map proto.')\n",
    "\n",
    "#\n",
    "FLAGS = flags.FLAGS\n",
    "training_path='L:/DataSet/Insight-MVT_Annotation_Train'\n",
    "\n",
    "\n",
    "data_dir_path='L:/DataSet'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tf_example(data,\n",
    "                       label_map_dict,\n",
    "                       image_subdirectory,\n",
    "                       bbox,\n",
    "                       ignore_difficult_instances=False):\n",
    "    img_path = os.path.join(image_subdirectory,data)\n",
    "    with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format not JPEG')\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    width,height = image.size\n",
    "\n",
    "    xmins = []\n",
    "    ymins = []\n",
    "    xmaxs = []\n",
    "    ymaxs = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    poses = []\n",
    "    difficult_obj = []\n",
    "\n",
    "  #  print(img_path)\n",
    "   \n",
    "    for target in bbox.findall('target'):\n",
    "        a = target.find('box')\n",
    "        b = target.find('attribute')\n",
    "       \n",
    "        \n",
    "        \n",
    "        xmin= float(a.attrib.get('left'))\n",
    "        ymin= float(a.attrib.get('top'))\n",
    "        xmax= float(a.attrib.get('width')) + xmin\n",
    "        ymax= float(a.attrib.get('height')) +ymin\n",
    "            \n",
    "        xmin= float(format(xmin, '.2f'))\n",
    "        ymin= float(format(ymin, '.2f'))\n",
    "        xmax= float(format(xmax, '.2f'))\n",
    "        ymax= float(format(ymax, '.2f'))\n",
    "        \n",
    "        xmins.append(xmin / width)\n",
    "        ymins.append(ymin / height)\n",
    "        xmaxs.append(xmax / width)\n",
    "        ymaxs.append(ymax / height)\n",
    "        class_name = b.attrib.get('vehicle_type')\n",
    "        classes_text.append(class_name.encode('utf8'))\n",
    "        classes.append(label_map_dict[class_name])\n",
    "        #    print('--> kao')\n",
    "         #   print(class_name)\n",
    "          #  print(xmin,ymin,xmax,ymax)\n",
    "      \n",
    "    \n",
    "    feature_dict = {\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(\n",
    "          data.encode('utf8')),\n",
    "      'image/source_id': dataset_util.bytes_feature(\n",
    "          data.encode('utf8')),\n",
    "      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "      }\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(label_map_dict,\n",
    "                     annotations_dir,\n",
    "                     image_dir,\n",
    "                     txt_dir,\n",
    "                     data_folder,\n",
    "                     ):\n",
    "\n",
    "\n",
    "    xml_path = os.path.join(annotations_dir, data_folder+'.xml')\n",
    "\n",
    "        \n",
    "        #with tf.gfile.GFile(xml_path, 'r') as fid:\n",
    "    \n",
    "            #xml_str = fid.read()\n",
    "        #xml = etree.fromstring(xml_str)\n",
    "        #data = dataset_util.recursive_parse_xml_to_dict(xml)\n",
    "    txtfile = dataset_util.read_examples_list(txt_dir+'/'+data_folder+'.txt')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    print(data_folder)\n",
    "    aa=[]  \n",
    "    img_list=[]\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    i=0 \n",
    "    j=0\n",
    "    for frame in root.findall('frame'):\n",
    "        num_img= int(float(frame.attrib.get('num')))\n",
    "        target_list = frame.find('target_list')\n",
    "        aa.append(target_list)\n",
    "        img_list.append(num_img)\n",
    "        #print(num_img)\n",
    "\n",
    "    Tfrec=[]\n",
    "    \n",
    "    try:\n",
    "           \n",
    "        for data in txtfile: \n",
    "            if((i+1)==img_list[j]):\n",
    "                Tfrec.append(dict_to_tf_example(data, label_map_dict, image_dir,aa[j]))\n",
    "                \n",
    "                if(j<len(img_list)-1):\n",
    "                    j=j+1\n",
    "            \n",
    "                #if (list_train_val[i]==0):\n",
    "                    #writer1 = tf.python_io.TFRecordWriter(val_output_path)\n",
    "                    #writer1.write(tf_example.SerializeToString())\n",
    "                    #writer1.close()\n",
    "                #else:\n",
    "                    #writer2 = tf.python_io.TFRecordWriter(train_output_path)\n",
    "                    #writer2.write(tf_example.SerializeToString())\n",
    "                    #writer2.close()\n",
    "            i=i+1\n",
    "    except ValueError:\n",
    "        logging.warning('Invalid example: %s, ignoring.', xml_path)\n",
    "\n",
    "    \n",
    "    return Tfrec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "MVI_20011\n",
      "train\n",
      "MVI_20012\n",
      "train\n",
      "MVI_20032\n",
      "train\n",
      "MVI_20033\n",
      "train\n",
      "MVI_20034\n",
      "train\n",
      "MVI_20035\n",
      "train\n",
      "MVI_20051\n",
      "train\n",
      "MVI_20052\n",
      "train\n",
      "MVI_20061\n",
      "train\n",
      "MVI_20062\n",
      "train\n",
      "MVI_20063\n",
      "train\n",
      "MVI_20064\n",
      "train\n",
      "MVI_20065\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(_):\n",
    "    data_dir = FLAGS.data_dir\n",
    "    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n",
    "\n",
    "   \n",
    "    logging.info('Reading from DETRAC dataset.')\n",
    "    \n",
    "    \n",
    "    annotations_dir = os.path.join(data_dir_path, 'DETRAC-Train-Annotations-XML')\n",
    "\n",
    "  # Test images are not included in the downloaded data set, so we shall perform\n",
    "  # our own split.\n",
    "\n",
    "\n",
    "    train_output_path = os.path.join(FLAGS.output_dir, 'detrac_train_only_1_final.record')\n",
    "    val_output_path = os.path.join(FLAGS.output_dir, 'detrac_val_only_1_final.record')\n",
    "    \n",
    "    txt_dir=data_dir_path+'/ua-text_file_front'\n",
    "    \n",
    "#    csv=pd.read_csv(excalibur+'/Insight-MVT_Annotation_Train.txt')\n",
    "    txtfile=dataset_util.read_examples_list(txt_dir+'/Insight-MVT_Annotation_Train.txt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    writer_val = tf.python_io.TFRecordWriter(val_output_path)\n",
    "    writer_train = tf.python_io.TFRecordWriter(train_output_path)\n",
    "    \n",
    "    i=0\n",
    "    num_folder=[]  \n",
    "   \n",
    "\n",
    "    \n",
    "    for data_folder in txtfile:\n",
    "        print('train')\n",
    "        image_dir = os.path.join(training_path, data_folder)\n",
    "        Tfrec=create_tf_record(label_map_dict, annotations_dir,image_dir,txt_dir,data_folder)\n",
    "        i+=1\n",
    "        \n",
    "        num_examples = len(Tfrec)\n",
    "        num_train = int(0.8 * num_examples)\n",
    "        random.seed(42)\n",
    "        random.shuffle(Tfrec)\n",
    "        count_train_val=0\n",
    "        \n",
    "        for a in Tfrec:\n",
    "            if(count_train_val<num_train):\n",
    "                writer_train.write(a.SerializeToString())#train \n",
    "            else:\n",
    "                writer_val.write(a.SerializeToString()) # val\n",
    "            count_train_val+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "  # TODO(user): Write code to read in your dataset to examples variable\n",
    "\n",
    "#  for example in examples:\n",
    "#    tf_example = create_tf_example(example)\n",
    "#    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "#  writer.close()\n",
    "\n",
    "    writer_train.close()\n",
    "    writer_val.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
